{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RNN作业\n",
    "\n",
    "本次作业我们将使用PyTorch搭建RNN模型，完成简单的文本分类任务。本次作业的选做题难度较大，仅供有自然语言处理基础且时间充裕的同学完成。\n",
    "\n",
    "截止时间：<font color=ff0000>**5.30(周一)中午** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 一、数据处理\n",
    "\n",
    "首先导入常用软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn import datasets, preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, MeanShift\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import numpy as np\n",
    "from torchtext.legacy import data\n",
    "import torchtext\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "本次作业使用的数据为torchtext.data中的AG_NEWS，每一行代表一个样本，三列数据分别代表：“类别”，“标题”，“正文”。该数据集中的类别有四种：“World”, “Sports”, “Business”, “Sci/Tech”，分别用数字1-4表示。\n",
    "\n",
    "因为原始数据集较大，截取了其中一部分作为训练集和测试集，train.csv中包含6600条数据，test.csv中包含1000条数据。\n",
    "\n",
    "使用torchtext完成数据的加载，主要使用以下三个组件：\n",
    "\n",
    "1. Field : 主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，起始字符，结束字符，补全字符以及词典等等\n",
    "\n",
    "2. Dataset : 继承自pytorch的Dataset，用于加载数据，提供了TabularDataset可以指点路径，格式，Field信息就可以方便的完成数据加载。同时torchtext还提供预先构建的常用数据集的Dataset对象，可以直接加载使用，splits方法可以同时加载训练集，验证集和测试集。\n",
    "\n",
    "3. Iterator : 主要是数据输出的模型的迭代器，可以支持batch定制\n",
    "\n",
    "如果在使用torchtext的过程中出现报错的情况，一种可能是由于版本问题，相关的类被移动到torchtext.legacy当中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '3', 'text': ['Unions', 'representing', 'workers', 'at', 'Turner', 'Newall', 'say', 'they', 'are', \"'disappointed'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul.']}\n",
      "{'label': '2', 'text': ['Six', 'players', 'from', 'both', 'Clemson', 'and', 'South', 'Carolina', 'will', 'be', 'suspended', 'for', 'one', 'game', 'next', 'season', 'for', 'their', 'participation', 'in', 'a', 'brawl', 'near', 'the', 'end', 'of', 'the', 'rivalry', 'game', 'November', '20th.']}\n"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.legacy.data.Field(sequential=True, batch_first=True, include_lengths=True)\n",
    "LABEL = torchtext.legacy.data.Field(sequential=False, batch_first=True, use_vocab=False)\n",
    "\n",
    "fields = [('label', LABEL), (None, None), ('text',TEXT)]  # 通过定义fields可以方便地读取数据，这里第一列为label，第三列为需要分类的text，标题暂时不需要\n",
    "\n",
    "train_data = torchtext.legacy.data.TabularDataset(path='train.csv', format='csv', fields=fields, skip_header=False)\n",
    "test_data = torchtext.legacy.data.TabularDataset(path='test.csv', format='csv', fields=fields, skip_header=False)\n",
    "#print preprocessed text\n",
    "print(vars(train_data.examples[0]))\n",
    "print(vars(test_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "torchtext提供了构建词表的功能：我们使用50维的预训练词向量，建立词典、索引以及对应的词向量映射关系。\n",
    "\n",
    "下载后的预训练词向量会默认存储在“.vector_cache”文件夹中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 8463\n",
      "Size of LABEL vocabulary: 5\n",
      "[('the', 9539), ('a', 5240), ('to', 5228), ('of', 4857), ('in', 4099), ('and', 3620), ('on', 2477), ('-', 2062), ('for', 2026), ('that', 1500)]\n"
     ]
    }
   ],
   "source": [
    "#initialize glove embeddings\n",
    "TEXT.build_vocab(train_data, min_freq=3, vectors=\"glove.6B.50d\")  # 去除低频词，使用50维的预训练词向量\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\", len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\", len(LABEL.vocab))  # 这里长度为5是因为build_vocabulary默认会添加一个<unk> token\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))  # tuple中第一个元素为单词，第二个元素为对应的索引\n",
    "\n",
    "#Word dictionary\n",
    "#print(TEXT.vocab.stoi)   # stoi可以将单词转换为数字索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 有条件的同学可以使用gpu加速训练\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# 划分之后的数据集，每个batch中有64个样本\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 词向量小练习\n",
    "\n",
    "glove.6B.50d.txt中每一行表示：单词 50维词向量\n",
    "\n",
    "gensim支持word2vec格式，因此需要先转换glove格式的词向量文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\92911\\AppData\\Local\\Temp/ipykernel_3692/1686190213.py:8: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  (count, dimensions) = glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# 格式转换\n",
    "glove_input_file = '.vector_cache/glove.6B.50d.txt'\n",
    "word2vec_output_file = '.vector_cache/glove.6B.50d.word2vec.txt'\n",
    "(count, dimensions) = glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "# 加载词向量\n",
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "词向量的加减可以得到有趣的结果，一个著名例子是公式：king - man + woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.759214460849762), ('daughter', 0.7473882436752319), ('elizabeth', 0.7460219860076904), ('princess', 0.7424570322036743), ('kingdom', 0.7337411642074585), ('monarch', 0.721449077129364), ('eldest', 0.7184861898422241), ('widow', 0.7099431157112122)]\n"
     ]
    }
   ],
   "source": [
    "result = glove_model.most_similar(positive=['woman', 'king'], negative=['man']) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**请自由尝试几个其他的词向量加减例子**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('daughter', 0.9612019062042236), ('wife', 0.906643807888031), ('mother', 0.9033514261245728), ('niece', 0.8949158191680908), ('granddaughter', 0.8855924606323242), ('father', 0.8785704970359802), ('married', 0.8772761225700378), ('grandmother', 0.8603695631027222), ('widow', 0.8590273857116699), ('cousin', 0.857356071472168)]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "result = glove_model.most_similar(positive=['son', 'woman'], negative=['man'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**请通过合适的方法将词向量降至2或3维，在空间坐标中显示出下列单词的位置，进行词向量可视化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = [\"cat\", \"dog\", \"fish\", \"kitten\", \"man\", \"woman\",\n",
    "         \"king\", \"queen\", \"doctor\", \"nurse\"]\n",
    "\n",
    "# TODO\n",
    "pre = glove_model[words]\n",
    "used = PCA(n_components=2)\n",
    "tmp = used.fit_transform(pre)\n",
    "plt.scatter(tmp[:, 0], tmp[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 二、RNN模型搭建\n",
    "\n",
    "这里参考课件上的代码搭建了一个简单的RNN模型\n",
    "\n",
    "1. 每次读取一个单词，因此RNN的输入维度为50（即词向量的维度）再加上hidden_size\n",
    "2. 最终的输出维度为4（对应四个类别）\n",
    "3. 初始隐状态可以用零向量\n",
    "\n",
    "有需要的话可以自由修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(embedding_dim + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_, hidden):\n",
    "        combined = torch.cat((input_, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)  # 初始化全0的隐状态向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 三、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_step(model, batch_data, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    # 载入数据\n",
    "    texts, text_lengths = batch_data.text  # texts当中存储的是单词的数字索引\n",
    "    labels = batch_data.label\n",
    "        \n",
    "    #print(labels)\n",
    "    #print(texts, text_lengths)\n",
    "    #print(len(text_lengths))\n",
    "    \n",
    "    for text, label in zip(texts, labels):\n",
    "        hidden = model.init_hidden().to(device)  # 创建一个初始隐状态\n",
    "        # 每次输入batch中的一个样本\n",
    "        for i in range(len(text)):\n",
    "            embedding = TEXT.vocab.vectors[text[i]]  # 根据text中的数字索引获得对应的词向量\n",
    "            embedding = embedding.to(device)\n",
    "            # print(embedding)\n",
    "            output, hidden = model(embedding.reshape(1, -1), hidden)  # 变换词向量的维度，使其能和hidden tensor拼接起来\n",
    "        target = (label - 1).unsqueeze(0)  # 原始label是1~4，减1之后对应下标0~3\n",
    "        loss = criterion(output, target)   # 计算loss\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if output.argmax(1) == label - 1:  # 判断模型预测的概率最大的类是否正确\n",
    "            total_acc += 1\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return total_loss / len(batch_data), total_acc / len(batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在训练过程中，我们希望能够保存效果最好的模型，使用save()和load()函数可以实现模型的保存和加载。\n",
    "\n",
    "参考文档：https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "**请实现：保存训练过程中loss最小的模型**\n",
    "\n",
    "考虑到训练阶段运行时间可能比较长，同学们可以自行选择合适的epoch数。本次作业旨在让大家熟悉基本模型结构，模型最终的表现效果不会成为评分指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, lr=0.01, num_epoch=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_arr = []\n",
    "    epoch_loss = []\n",
    "    # set the model in training phase\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in tqdm(range(num_epoch)):\n",
    "        mean_acc = []\n",
    "        mean_loss = []\n",
    "        for i, batch_data in enumerate(iterator):\n",
    "            batch_loss, batch_acc = train_step(model, batch_data, optimizer, criterion)\n",
    "            loss_arr.append(batch_loss)\n",
    "            mean_loss.append(batch_loss)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(\"Iteration number:\", i + 1,'Loss:', batch_loss, \"Acc:\", batch_acc)\n",
    "            mean_acc.append(batch_acc)\n",
    "        \n",
    "        print(\"Epoch Acc:\", np.mean(mean_acc), \"Epoch loss\", np.mean(mean_loss))\n",
    "\n",
    "        # TODO: 保存loss最小的模型\n",
    "        epoch_loss.append(np.mean(mean_loss))\n",
    "        if np.mean(mean_loss) <= np.min(epoch_loss):\n",
    "            torch.save(model.state_dict(),'modelparam')\n",
    "        \n",
    "    \n",
    "    # 绘制loss曲线\n",
    "    plt.figure()\n",
    "    plt.plot(loss_arr, \"-*\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 10 Loss: 1.352486478164792 Acc: 0.328125\n",
      "Iteration number: 20 Loss: 1.326682941056788 Acc: 0.421875\n",
      "Iteration number: 30 Loss: 1.2676816210150719 Acc: 0.421875\n",
      "Iteration number: 40 Loss: 0.9986333844717592 Acc: 0.609375\n",
      "Iteration number: 50 Loss: 0.9224279809591776 Acc: 0.546875\n",
      "Iteration number: 60 Loss: 0.7183096126154851 Acc: 0.703125\n",
      "Iteration number: 70 Loss: 0.9983469783328474 Acc: 0.625\n",
      "Iteration number: 80 Loss: 1.2566308546811342 Acc: 0.421875\n",
      "Iteration number: 90 Loss: 0.9561358471401036 Acc: 0.546875\n",
      "Iteration number: 100 Loss: 1.0056687770373962 Acc: 0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 1/10 [00:32<04:56, 32.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.5320012019230769 Epoch loss 1.0441242310826018\n",
      "Iteration number: 10 Loss: 0.8204774440382607 Acc: 0.671875\n",
      "Iteration number: 20 Loss: 0.7405929753076634 Acc: 0.625\n",
      "Iteration number: 30 Loss: 0.9101921302726055 Acc: 0.640625\n",
      "Iteration number: 40 Loss: 1.2183565539307892 Acc: 0.390625\n",
      "Iteration number: 50 Loss: 1.1617222712375224 Acc: 0.484375\n",
      "Iteration number: 60 Loss: 1.1948953701648861 Acc: 0.46875\n",
      "Iteration number: 70 Loss: 1.124975296203047 Acc: 0.5625\n",
      "Iteration number: 80 Loss: 1.1032307678833604 Acc: 0.625\n",
      "Iteration number: 90 Loss: 1.177717283833772 Acc: 0.453125\n",
      "Iteration number: 100 Loss: 1.2354629142209888 Acc: 0.515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [01:05<04:22, 32.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.5172776442307693 Epoch loss 1.0941742171150572\n",
      "Iteration number: 10 Loss: 1.1449258383363485 Acc: 0.453125\n",
      "Iteration number: 20 Loss: 1.0436970966402441 Acc: 0.625\n",
      "Iteration number: 30 Loss: 0.9749948516255245 Acc: 0.609375\n",
      "Iteration number: 40 Loss: 0.9229581379913725 Acc: 0.671875\n",
      "Iteration number: 50 Loss: 0.9786398109281436 Acc: 0.578125\n",
      "Iteration number: 60 Loss: 0.818479654204566 Acc: 0.6875\n",
      "Iteration number: 70 Loss: 0.6869586984394118 Acc: 0.6875\n",
      "Iteration number: 80 Loss: 0.8601864181982819 Acc: 0.59375\n",
      "Iteration number: 90 Loss: 0.8780429134640144 Acc: 0.5625\n",
      "Iteration number: 100 Loss: 0.6804747848509578 Acc: 0.765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [01:37<03:45, 32.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.5862379807692307 Epoch loss 0.961724927700507\n",
      "Iteration number: 10 Loss: 0.8491950734751299 Acc: 0.65625\n",
      "Iteration number: 20 Loss: 0.6854450039027142 Acc: 0.75\n",
      "Iteration number: 30 Loss: 0.7296826975070871 Acc: 0.75\n",
      "Iteration number: 40 Loss: 0.7499424343113787 Acc: 0.71875\n",
      "Iteration number: 50 Loss: 0.6641941064044659 Acc: 0.703125\n",
      "Iteration number: 60 Loss: 0.74025005033036 Acc: 0.703125\n",
      "Iteration number: 70 Loss: 0.8941821616026573 Acc: 0.625\n",
      "Iteration number: 80 Loss: 0.7373367840773426 Acc: 0.671875\n",
      "Iteration number: 90 Loss: 0.7845947383761995 Acc: 0.640625\n",
      "Iteration number: 100 Loss: 0.7485247830918524 Acc: 0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [02:08<03:12, 32.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.6684194711538461 Epoch loss 0.8165320788772983\n",
      "Iteration number: 10 Loss: 0.6667130154382903 Acc: 0.765625\n",
      "Iteration number: 20 Loss: 0.5484345566164848 Acc: 0.75\n",
      "Iteration number: 30 Loss: 0.5928865116038651 Acc: 0.765625\n",
      "Iteration number: 40 Loss: 0.720816370096145 Acc: 0.703125\n",
      "Iteration number: 50 Loss: 0.6790656763005245 Acc: 0.75\n",
      "Iteration number: 60 Loss: 0.6622623446110083 Acc: 0.75\n",
      "Iteration number: 70 Loss: 0.6408080885103118 Acc: 0.8125\n",
      "Iteration number: 80 Loss: 0.787464671604539 Acc: 0.765625\n",
      "Iteration number: 90 Loss: 0.5026494972698856 Acc: 0.828125\n",
      "Iteration number: 100 Loss: 0.6084022070581341 Acc: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [02:40<02:38, 31.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.7295673076923077 Epoch loss 0.7109700828076224\n",
      "Iteration number: 10 Loss: 0.5519822322385153 Acc: 0.78125\n",
      "Iteration number: 20 Loss: 0.6400056904722078 Acc: 0.828125\n",
      "Iteration number: 30 Loss: 0.5012638408181829 Acc: 0.8125\n",
      "Iteration number: 40 Loss: 0.5462441904383013 Acc: 0.796875\n",
      "Iteration number: 50 Loss: 0.8310108640580438 Acc: 0.75\n",
      "Iteration number: 60 Loss: 0.7043104772619699 Acc: 0.6875\n",
      "Iteration number: 70 Loss: 0.5408395089432361 Acc: 0.796875\n",
      "Iteration number: 80 Loss: 0.8589706433413085 Acc: 0.734375\n",
      "Iteration number: 90 Loss: 0.8601559253584128 Acc: 0.703125\n",
      "Iteration number: 100 Loss: 0.8283165339380503 Acc: 0.640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [03:12<02:07, 31.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.6950120192307693 Epoch loss 0.7539936317034528\n",
      "Iteration number: 10 Loss: 0.816774177190382 Acc: 0.6875\n",
      "Iteration number: 20 Loss: 0.9274444585898891 Acc: 0.5625\n",
      "Iteration number: 30 Loss: 0.9633872676058672 Acc: 0.703125\n",
      "Iteration number: 40 Loss: 0.7367925399448723 Acc: 0.6875\n",
      "Iteration number: 50 Loss: 0.7543064185156254 Acc: 0.671875\n",
      "Iteration number: 60 Loss: 0.6982553353300318 Acc: 0.765625\n",
      "Iteration number: 70 Loss: 0.743929676595144 Acc: 0.65625\n",
      "Iteration number: 80 Loss: 0.7883108797432214 Acc: 0.65625\n",
      "Iteration number: 90 Loss: 0.7190674769449288 Acc: 0.765625\n",
      "Iteration number: 100 Loss: 0.7000698311803717 Acc: 0.765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [03:45<01:36, 32.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.6651141826923077 Epoch loss 0.8191830792260668\n",
      "Iteration number: 10 Loss: 0.6826186053876881 Acc: 0.796875\n",
      "Iteration number: 20 Loss: 0.7156027586671598 Acc: 0.75\n",
      "Iteration number: 30 Loss: 0.5559330924415917 Acc: 0.765625\n",
      "Iteration number: 40 Loss: 0.858961079198707 Acc: 0.71875\n",
      "Iteration number: 50 Loss: 0.600592476911288 Acc: 0.78125\n",
      "Iteration number: 60 Loss: 0.5973855518514029 Acc: 0.734375\n",
      "Iteration number: 70 Loss: 0.6855879981957145 Acc: 0.78125\n",
      "Iteration number: 80 Loss: 0.6704861142738991 Acc: 0.75\n",
      "Iteration number: 90 Loss: 0.5736353972403094 Acc: 0.8125\n",
      "Iteration number: 100 Loss: 0.8415496062880266 Acc: 0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [04:17<01:04, 32.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.7307692307692307 Epoch loss 0.7046629664420371\n",
      "Iteration number: 10 Loss: 0.6874974432621457 Acc: 0.703125\n",
      "Iteration number: 20 Loss: 0.6775937050026073 Acc: 0.765625\n"
     ]
    }
   ],
   "source": [
    "# RNN训练 具体的参数可以自行调整\n",
    "n_hidden = 128\n",
    "n_input = 50\n",
    "n_output = 4\n",
    "rnn = RNN(n_input, n_hidden, n_output)\n",
    "rnn.to(device)\n",
    "train(rnn, train_iterator, lr=0.0005, num_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 四、结果评测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "计算RNN模型在测试集上的准确率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def eval_test(model, iterator):\n",
    "    total_cnt = 0\n",
    "    total_acc = 0\n",
    "    for batch_data in iterator:\n",
    "        texts, text_lengths = batch_data.text\n",
    "        labels = batch_data.label\n",
    "        \n",
    "        for text, label in zip(texts, labels):\n",
    "            hidden = model.init_hidden().to(device)\n",
    "            for i in range(len(text)):\n",
    "                embedding = TEXT.vocab.vectors[text[i]]\n",
    "                embedding = embedding.to(device)\n",
    "                output, hidden = model(embedding.reshape(1, -1), hidden)\n",
    "            \n",
    "            if output.argmax(1) == label - 1:  # 判断预测概率最大的类是否正确\n",
    "                total_acc += 1\n",
    "            total_cnt += 1\n",
    "\n",
    "    print(\"Test Acc:\", total_acc / total_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**请加载效果最好的模型，输出模型在测试集上的准确率**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.752\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "rnn.load_state_dict(torch.load('modelparam'))\n",
    "eval_test(rnn,test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 五、附加部分（1'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**可选1 模型优化**：在上述文本分类任务中，只使用了文章的正文内容，没有利用标题中的信息。请尝试加入标题内容，修改模型，以优化模型的分类能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**可选2 对抗实验**：训练完成后，我们可以得到一个模型能够正确预测的样本构成的集合，请在已有的模型能够正确分类的样本集中设计一些对抗实验。\n",
    "\n",
    "例如：对原始的输入文本随机删去一些词，或是将部分词遮盖（替换）成&lt;unk&gt;，表示未知词。（前面用torchtext构建的TEXT.vocab词表中第一个token是&lt;unk&gt;）\n",
    "\n",
    "这部分可以设置一个适当的比例，比如遮盖/删除20%的词。计算模型对于这样的输入的预测错误率，并尝试改进策略，使得模型在面对“20%的词被遮盖”（或者其他条件）的情况时，错误预测率尽可能低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
