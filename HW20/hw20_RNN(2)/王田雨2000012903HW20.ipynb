{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN作业\n",
    "\n",
    "本次作业我们将使用PyTorch搭建RNN模型，完成简单的文本分类任务。本次作业的选做题难度较大，仅供有自然语言处理基础且时间充裕的同学完成。\n",
    "\n",
    "截止时间：<font color=ff0000>**5.30(周一)中午** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、数据处理\n",
    "\n",
    "首先导入常用软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_asset_local_path' from 'torchtext.utils' (C:\\Users\\92911\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchtext\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12892/1486612157.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#import torchtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchtext\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchtext\\transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_sp_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_asset_local_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_torchtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGPT2BPEEncoder\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mGPT2BPEEncoderPyBind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLIPEncoder\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mCLIPEncoderPyBind\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_asset_local_path' from 'torchtext.utils' (C:\\Users\\92911\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchtext\\utils.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn import datasets, preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, MeanShift\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import numpy as np\n",
    "#from torchtext.legacy import data\n",
    "#import torchtext\n",
    "from tqdm import tqdm\n",
    "#import torchtext\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业使用的数据为torchtext.data中的AG_NEWS，每一行代表一个样本，三列数据分别代表：“类别”，“标题”，“正文”。该数据集中的类别有四种：“World”, “Sports”, “Business”, “Sci/Tech”，分别用数字1-4表示。\n",
    "\n",
    "因为原始数据集较大，截取了其中一部分作为训练集和测试集，train.csv中包含6600条数据，test.csv中包含1000条数据。\n",
    "\n",
    "使用torchtext完成数据的加载，主要使用以下三个组件：\n",
    "\n",
    "1. Field : 主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，起始字符，结束字符，补全字符以及词典等等\n",
    "\n",
    "2. Dataset : 继承自pytorch的Dataset，用于加载数据，提供了TabularDataset可以指点路径，格式，Field信息就可以方便的完成数据加载。同时torchtext还提供预先构建的常用数据集的Dataset对象，可以直接加载使用，splits方法可以同时加载训练集，验证集和测试集。\n",
    "\n",
    "3. Iterator : 主要是数据输出的模型的迭代器，可以支持batch定制\n",
    "\n",
    "如果在使用torchtext的过程中出现报错的情况，一种可能是由于版本问题，相关的类被移动到torchtext.legacy当中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '3', 'text': ['Unions', 'representing', 'workers', 'at', 'Turner', 'Newall', 'say', 'they', 'are', \"'disappointed'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul.']}\n",
      "{'label': '2', 'text': ['Six', 'players', 'from', 'both', 'Clemson', 'and', 'South', 'Carolina', 'will', 'be', 'suspended', 'for', 'one', 'game', 'next', 'season', 'for', 'their', 'participation', 'in', 'a', 'brawl', 'near', 'the', 'end', 'of', 'the', 'rivalry', 'game', 'November', '20th.']}\n"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.legacy.data.Field(sequential=True, batch_first=True, include_lengths=True)\n",
    "LABEL = torchtext.legacy.data.Field(sequential=False, batch_first=True, use_vocab=False)\n",
    "\n",
    "fields = [('label', LABEL), (None, None), ('text',TEXT)]  # 通过定义fields可以方便地读取数据，这里第一列为label，第三列为需要分类的text，标题暂时不需要\n",
    "\n",
    "train_data = data.TabularDataset(path='train.csv', format='csv', fields=fields, skip_header=False)\n",
    "test_data = data.TabularDataset(path='test.csv', format='csv', fields=fields, skip_header=False)\n",
    "#print preprocessed text\n",
    "print(vars(train_data.examples[0]))\n",
    "print(vars(test_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtext提供了构建词表的功能：我们使用50维的预训练词向量，建立词典、索引以及对应的词向量映射关系。\n",
    "\n",
    "下载后的预训练词向量会默认存储在“.vector_cache”文件夹中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████▉| 399999/400000 [00:08<00:00, 45269.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 8463\n",
      "Size of LABEL vocabulary: 5\n",
      "[('the', 9539), ('a', 5240), ('to', 5228), ('of', 4857), ('in', 4099), ('and', 3620), ('on', 2477), ('-', 2062), ('for', 2026), ('that', 1500)]\n"
     ]
    }
   ],
   "source": [
    "#initialize glove embeddings\n",
    "TEXT.build_vocab(train_data, min_freq=3, vectors=\"glove.6B.50d\")  # 去除低频词，使用50维的预训练词向量\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\", len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\", len(LABEL.vocab))  # 这里长度为5是因为build_vocabulary默认会添加一个<unk> token\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))  # tuple中第一个元素为单词，第二个元素为对应的索引\n",
    "\n",
    "#Word dictionary\n",
    "#print(TEXT.vocab.stoi)   # stoi可以将单词转换为数字索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 有条件的同学可以使用gpu加速训练\n",
    "print(device)\n",
    "batch_size = 64\n",
    "\n",
    "# 划分之后的数据集，每个batch中有64个样本\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量小练习\n",
    "\n",
    "glove.6B.50d.txt中每一行表示：单词 50维词向量\n",
    "\n",
    "gensim支持word2vec格式，因此需要先转换glove格式的词向量文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\92911\\AppData\\Local\\Temp/ipykernel_3284/1686190213.py:8: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  (count, dimensions) = glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# 格式转换\n",
    "glove_input_file = '.vector_cache/glove.6B.50d.txt'\n",
    "word2vec_output_file = '.vector_cache/glove.6B.50d.word2vec.txt'\n",
    "(count, dimensions) = glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "# 加载词向量\n",
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词向量的加减可以得到有趣的结果，一个著名例子是公式：king - man + woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.759214460849762), ('daughter', 0.7473882436752319), ('elizabeth', 0.7460219860076904), ('princess', 0.7424570322036743), ('kingdom', 0.7337411642074585), ('monarch', 0.721449077129364), ('eldest', 0.7184861898422241), ('widow', 0.7099431157112122)]\n"
     ]
    }
   ],
   "source": [
    "result = glove_model.most_similar(positive=['woman', 'king'], negative=['man']) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**请自由尝试几个其他的词向量加减例子**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('soldier', 0.7302291989326477), ('policeman', 0.7214615941047668), ('person', 0.7156468033790588), ('whose', 0.7109968662261963), ('who', 0.710432767868042), ('citizen', 0.7102181315422058), ('an', 0.7093256115913391), ('old', 0.7072909474372864), ('journalist', 0.7053940892219543), ('victim', 0.7044424414634705)]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "result = glove_model.most_similar(positive=['man', 'woman'], negative=['baby']) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**请通过合适的方法将词向量降至2或3维，在空间坐标中显示出下列单词的位置，进行词向量可视化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = [\"cat\", \"dog\", \"fish\", \"kitten\", \"man\", \"woman\",\n",
    "         \"king\", \"queen\", \"doctor\", \"nurse\"]\n",
    "vec = glove_model[words]\n",
    "pca2d = PCA(n_components=2)\n",
    "vec_2d = pca2d.fit_transform(vec)\n",
    "plt.scatter(vec_2d[:, 0], vec_2d[:, 1])\n",
    "plt.show()\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、RNN模型搭建\n",
    "\n",
    "这里参考课件上的代码搭建了一个简单的RNN模型\n",
    "\n",
    "1. 每次读取一个单词，因此RNN的输入维度为50（即词向量的维度）再加上hidden_size\n",
    "2. 最终的输出维度为4（对应四个类别）\n",
    "3. 初始隐状态可以用零向量\n",
    "\n",
    "有需要的话可以自由修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(embedding_dim + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_, hidden):\n",
    "        combined = torch.cat((input_, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)  # 初始化全0的隐状态向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, batch_data, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    # 载入数据\n",
    "    texts, text_lengths = batch_data.text  # texts当中存储的是单词的数字索引\n",
    "    labels = batch_data.label\n",
    "        \n",
    "    #print(labels)\n",
    "    #print(texts, text_lengths)\n",
    "    #print(len(text_lengths))\n",
    "    \n",
    "    for text, label in zip(texts, labels):\n",
    "        hidden = model.init_hidden().to(device)  # 创建一个初始隐状态\n",
    "        # 每次输入batch中的一个样本\n",
    "        for i in range(len(text)):\n",
    "            embedding = TEXT.vocab.vectors[text[i]]  # 根据text中的数字索引获得对应的词向量\n",
    "            embedding = embedding.to(device)\n",
    "            # print(embedding)\n",
    "            output, hidden = model(embedding.reshape(1, -1), hidden)  # 变换词向量的维度，使其能和hidden tensor拼接起来\n",
    "        target = (label - 1).unsqueeze(0)  # 原始label是1~4，减1之后对应下标0~3\n",
    "        loss = criterion(output, target)   # 计算loss\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if output.argmax(1) == label - 1:  # 判断模型预测的概率最大的类是否正确\n",
    "            total_acc += 1\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return total_loss / len(batch_data), total_acc / len(batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练过程中，我们希望能够保存效果最好的模型，使用save()和load()函数可以实现模型的保存和加载。\n",
    "\n",
    "参考文档：https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "**请实现：保存训练过程中loss最小的模型**\n",
    "\n",
    "考虑到训练阶段运行时间可能比较长，同学们可以自行选择合适的epoch数。本次作业旨在让大家熟悉基本模型结构，模型最终的表现效果不会成为评分指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, lr=0.01, num_epoch=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_arr = []\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = []\n",
    "    # set the model in training phase\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in tqdm(range(num_epoch)):\n",
    "        mean_acc = []\n",
    "        mean_loss = []\n",
    "        for i, batch_data in enumerate(iterator):\n",
    "            batch_loss, batch_acc = train_step(model, batch_data, optimizer, criterion)\n",
    "            loss_arr.append(batch_loss)\n",
    "            mean_loss.append(batch_loss)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(\"Iteration number:\", i + 1,'Loss:', batch_loss, \"Acc:\", batch_acc)\n",
    "            mean_acc.append(batch_acc)\n",
    "        \n",
    "        print(\"Epoch Acc:\", np.mean(mean_acc), \"Epoch loss\", np.mean(mean_loss))\n",
    "        epoch_loss.append(np.mean(mean_loss))\n",
    "        if np.mean(mean_loss) <= np.min(epoch_loss):\n",
    "            torch.save(model.state_dict(),'modelparam')\n",
    "        # TODO: 保存loss最小的模型\n",
    "        \n",
    "    \n",
    "    # 绘制loss曲线\n",
    "    plt.figure()\n",
    "    plt.plot(loss_arr, \"-*\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 10 Loss: 1.3582249004393816 Acc: 0.4375\n",
      "Iteration number: 20 Loss: 1.2879359256476164 Acc: 0.484375\n",
      "Iteration number: 30 Loss: 1.1328289518132806 Acc: 0.515625\n",
      "Iteration number: 40 Loss: 1.1718104183673859 Acc: 0.421875\n",
      "Iteration number: 50 Loss: 1.2101775296032429 Acc: 0.546875\n",
      "Iteration number: 60 Loss: 1.2067246418446302 Acc: 0.59375\n",
      "Iteration number: 70 Loss: 1.1002354361116886 Acc: 0.578125\n",
      "Iteration number: 80 Loss: 0.9535155158955604 Acc: 0.65625\n",
      "Iteration number: 90 Loss: 0.8647242281585932 Acc: 0.640625\n",
      "Iteration number: 100 Loss: 0.8192986436188221 Acc: 0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 1/10 [00:31<04:45, 31.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.4905348557692308 Epoch loss 1.1738800677023977\n",
      "Iteration number: 10 Loss: 0.8233617676887661 Acc: 0.625\n",
      "Iteration number: 20 Loss: 0.9111826680600643 Acc: 0.578125\n",
      "Iteration number: 30 Loss: 1.0827910954249091 Acc: 0.5625\n",
      "Iteration number: 40 Loss: 0.8910358615976293 Acc: 0.71875\n",
      "Iteration number: 50 Loss: 0.7541468360432191 Acc: 0.71875\n",
      "Iteration number: 60 Loss: 0.727440782065969 Acc: 0.765625\n",
      "Iteration number: 70 Loss: 0.6720356267614989 Acc: 0.75\n",
      "Iteration number: 80 Loss: 0.6230192294779044 Acc: 0.765625\n",
      "Iteration number: 90 Loss: 0.7090892410233209 Acc: 0.671875\n",
      "Iteration number: 100 Loss: 0.8402167295571417 Acc: 0.640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [01:04<04:18, 32.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.6888521634615384 Epoch loss 0.7792075502893349\n",
      "Iteration number: 10 Loss: 1.0248039732687175 Acc: 0.53125\n",
      "Iteration number: 20 Loss: 0.8153118327027187 Acc: 0.703125\n",
      "Iteration number: 30 Loss: 0.8226947735820431 Acc: 0.6875\n",
      "Iteration number: 40 Loss: 0.6507266504904692 Acc: 0.796875\n",
      "Iteration number: 50 Loss: 0.6831185187511437 Acc: 0.71875\n",
      "Iteration number: 60 Loss: 0.7768117570572031 Acc: 0.71875\n",
      "Iteration number: 70 Loss: 0.6966715374437626 Acc: 0.75\n",
      "Iteration number: 80 Loss: 0.6374692092213081 Acc: 0.78125\n",
      "Iteration number: 90 Loss: 0.5672032643778948 Acc: 0.8125\n",
      "Iteration number: 100 Loss: 0.4750758173695431 Acc: 0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [01:38<03:52, 33.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.7235576923076923 Epoch loss 0.7131161464693141\n",
      "Iteration number: 10 Loss: 0.7333508084120695 Acc: 0.75\n",
      "Iteration number: 20 Loss: 0.6847804414574625 Acc: 0.765625\n",
      "Iteration number: 30 Loss: 0.7344710171200859 Acc: 0.71875\n",
      "Iteration number: 40 Loss: 0.9794348999275826 Acc: 0.5625\n",
      "Iteration number: 50 Loss: 0.7994022208731622 Acc: 0.671875\n",
      "Iteration number: 60 Loss: 0.599206782906549 Acc: 0.75\n",
      "Iteration number: 70 Loss: 0.593338728434901 Acc: 0.78125\n",
      "Iteration number: 80 Loss: 0.7594232520391415 Acc: 0.78125\n",
      "Iteration number: 90 Loss: 0.47496882366522186 Acc: 0.796875\n",
      "Iteration number: 100 Loss: 0.544275153532908 Acc: 0.828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [02:13<03:22, 33.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.7445913461538461 Epoch loss 0.6840619327719248\n",
      "Iteration number: 10 Loss: 0.883921990825911 Acc: 0.6875\n",
      "Iteration number: 20 Loss: 0.8513377249619225 Acc: 0.625\n",
      "Iteration number: 30 Loss: 0.9042854363069637 Acc: 0.609375\n",
      "Iteration number: 40 Loss: 0.9042480922735194 Acc: 0.6875\n",
      "Iteration number: 50 Loss: 0.6175349516615825 Acc: 0.75\n",
      "Iteration number: 60 Loss: 0.601663088420537 Acc: 0.859375\n",
      "Iteration number: 70 Loss: 0.8392540023924084 Acc: 0.609375\n",
      "Iteration number: 80 Loss: 0.6915124676088453 Acc: 0.734375\n",
      "Iteration number: 90 Loss: 0.6838806370506063 Acc: 0.75\n",
      "Iteration number: 100 Loss: 0.5862296083068941 Acc: 0.78125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [02:48<02:50, 34.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.7211538461538461 Epoch loss 0.802154384182932\n",
      "Iteration number: 10 Loss: 0.5867168723380018 Acc: 0.8125\n",
      "Iteration number: 20 Loss: 0.5511421402370615 Acc: 0.734375\n",
      "Iteration number: 30 Loss: 0.6387176781572634 Acc: 0.8125\n",
      "Iteration number: 40 Loss: 0.5664669449615758 Acc: 0.796875\n",
      "Iteration number: 50 Loss: 0.5394919585451134 Acc: 0.75\n",
      "Iteration number: 60 Loss: 0.8021511890183319 Acc: 0.65625\n",
      "Iteration number: 70 Loss: 0.7005087997349619 Acc: 0.75\n",
      "Iteration number: 80 Loss: 0.6991572334900411 Acc: 0.71875\n",
      "Iteration number: 90 Loss: 0.533888457044668 Acc: 0.8125\n",
      "Iteration number: 100 Loss: 0.6086728182381194 Acc: 0.796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [03:22<02:17, 34.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.7372295673076923 Epoch loss 0.6918454280452533\n",
      "Iteration number: 10 Loss: 0.8127921227496699 Acc: 0.75\n",
      "Iteration number: 20 Loss: 1.3044212706154212 Acc: 0.453125\n",
      "Iteration number: 30 Loss: 0.7542767905397341 Acc: 0.765625\n",
      "Iteration number: 40 Loss: 0.8726849578088149 Acc: 0.71875\n",
      "Iteration number: 50 Loss: 0.822772563144099 Acc: 0.65625\n",
      "Iteration number: 60 Loss: 0.6344417187501676 Acc: 0.8125\n",
      "Iteration number: 70 Loss: 0.6528625004830246 Acc: 0.734375\n",
      "Iteration number: 80 Loss: 0.6280531286428186 Acc: 0.78125\n",
      "Iteration number: 90 Loss: 0.7465076598978158 Acc: 0.65625\n",
      "Iteration number: 100 Loss: 0.9301504997711163 Acc: 0.671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [03:56<01:42, 34.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.73046875 Epoch loss 0.7198451855168531\n",
      "Iteration number: 10 Loss: 0.7392014927463606 Acc: 0.671875\n",
      "Iteration number: 20 Loss: 0.4764008364144843 Acc: 0.84375\n",
      "Iteration number: 30 Loss: 0.7553027144203952 Acc: 0.703125\n",
      "Iteration number: 40 Loss: 0.612875765350509 Acc: 0.71875\n",
      "Iteration number: 50 Loss: 0.6188039840017154 Acc: 0.78125\n",
      "Iteration number: 60 Loss: 0.6960113367940721 Acc: 0.796875\n",
      "Iteration number: 70 Loss: 0.506441058717428 Acc: 0.765625\n",
      "Iteration number: 80 Loss: 0.7033764909720048 Acc: 0.734375\n",
      "Iteration number: 90 Loss: 0.6069274684795687 Acc: 0.734375\n",
      "Iteration number: 100 Loss: 0.7257413003535476 Acc: 0.640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [04:32<01:09, 34.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.7475961538461539 Epoch loss 0.6761874630241695\n",
      "Iteration number: 10 Loss: 0.5420578579735889 Acc: 0.765625\n",
      "Iteration number: 20 Loss: 1.2553167501464486 Acc: 0.453125\n",
      "Iteration number: 30 Loss: 1.117213211255148 Acc: 0.5\n",
      "Iteration number: 40 Loss: 1.156370238168165 Acc: 0.515625\n",
      "Iteration number: 50 Loss: 1.1345088426023722 Acc: 0.53125\n",
      "Iteration number: 60 Loss: 1.137475413037464 Acc: 0.546875\n",
      "Iteration number: 70 Loss: 1.2014823746867478 Acc: 0.453125\n",
      "Iteration number: 80 Loss: 1.0674511257675476 Acc: 0.46875\n",
      "Iteration number: 90 Loss: 1.0241967826150358 Acc: 0.640625\n",
      "Iteration number: 100 Loss: 1.0610235288040712 Acc: 0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [05:06<00:34, 34.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.5264423076923077 Epoch loss 1.1142622778816562\n",
      "Iteration number: 10 Loss: 1.0923953328747302 Acc: 0.5625\n",
      "Iteration number: 20 Loss: 0.9900752868270501 Acc: 0.703125\n",
      "Iteration number: 30 Loss: 1.081723992334446 Acc: 0.53125\n",
      "Iteration number: 40 Loss: 0.9538579968211707 Acc: 0.53125\n",
      "Iteration number: 50 Loss: 0.9246253275086929 Acc: 0.5625\n",
      "Iteration number: 60 Loss: 1.0626008382532746 Acc: 0.578125\n",
      "Iteration number: 70 Loss: 0.9358697295174352 Acc: 0.609375\n",
      "Iteration number: 80 Loss: 0.7825511138944421 Acc: 0.75\n",
      "Iteration number: 90 Loss: 0.712780369023676 Acc: 0.734375\n",
      "Iteration number: 100 Loss: 0.6762254803034011 Acc: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [05:41<00:00, 34.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Acc: 0.6200420673076923 Epoch loss 0.9154964074704839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RNN训练 具体的参数可以自行调整\n",
    "n_hidden = 128\n",
    "n_input = 50\n",
    "n_output = 4\n",
    "rnn = RNN(n_input, n_hidden, n_output)\n",
    "rnn.to(device)\n",
    "train(rnn, train_iterator, lr=0.0005, num_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、结果评测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算RNN模型在测试集上的准确率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(model, iterator):\n",
    "    total_cnt = 0\n",
    "    total_acc = 0\n",
    "    for batch_data in iterator:\n",
    "        texts, text_lengths = batch_data.text\n",
    "        labels = batch_data.label\n",
    "        \n",
    "        for text, label in zip(texts, labels):\n",
    "            hidden = model.init_hidden().to(device)\n",
    "            for i in range(len(text)):\n",
    "                embedding = TEXT.vocab.vectors[text[i]]\n",
    "                embedding = embedding.to(device)\n",
    "                output, hidden = model(embedding.reshape(1, -1), hidden)\n",
    "            \n",
    "            if output.argmax(1) == label - 1:  # 判断预测概率最大的类是否正确\n",
    "                total_acc += 1\n",
    "            total_cnt += 1\n",
    "\n",
    "    print(\"Test Acc:\", total_acc / total_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**请加载效果最好的模型，输出模型在测试集上的准确率**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.752\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "rnn.load_state_dict(torch.load('modelparam'))\n",
    "eval_test(rnn,test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、附加部分（1'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**可选1 模型优化**：在上述文本分类任务中，只使用了文章的正文内容，没有利用标题中的信息。请尝试加入标题内容，修改模型，以优化模型的分类能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**可选2 对抗实验**：训练完成后，我们可以得到一个模型能够正确预测的样本构成的集合，请在已有的模型能够正确分类的样本集中设计一些对抗实验。\n",
    "\n",
    "例如：对原始的输入文本随机删去一些词，或是将部分词遮盖（替换）成&lt;unk&gt;，表示未知词。（前面用torchtext构建的TEXT.vocab词表中第一个token是&lt;unk&gt;）\n",
    "\n",
    "这部分可以设置一个适当的比例，比如遮盖/删除20%的词。计算模型对于这样的输入的预测错误率，并尝试改进策略，使得模型在面对“20%的词被遮盖”（或者其他条件）的情况时，错误预测率尽可能低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
